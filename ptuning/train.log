nohup: ignoring inputmaster_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.09/03/2023 10:30:11 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False09/03/2023 10:30:11 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(_n_gpu=1,adafactor=False,adam_beta1=0.9,adam_beta2=0.999,adam_epsilon=1e-08,auto_find_batch_size=False,bf16=False,bf16_full_eval=False,data_seed=None,dataloader_drop_last=False,dataloader_num_workers=0,dataloader_pin_memory=True,ddp_backend=None,ddp_bucket_cap_mb=None,ddp_find_unused_parameters=None,ddp_timeout=1800,debug=[],deepspeed=None,disable_tqdm=False,do_eval=False,do_predict=False,do_train=True,eval_accumulation_steps=None,eval_delay=0,eval_steps=None,evaluation_strategy=no,fp16=False,fp16_backend=auto,fp16_full_eval=False,fp16_opt_level=O1,fsdp=[],fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},fsdp_min_num_params=0,fsdp_transformer_layer_cls_to_wrap=None,full_determinism=False,generation_config=None,generation_max_length=None,generation_num_beams=None,gradient_accumulation_steps=16,gradient_checkpointing=False,greater_is_better=None,group_by_length=False,half_precision_backend=auto,hub_model_id=None,hub_private_repo=False,hub_strategy=every_save,hub_token=<HUB_TOKEN>,ignore_data_skip=False,include_inputs_for_metrics=False,jit_mode_eval=False,label_names=None,label_smoothing_factor=0.0,learning_rate=0.02,length_column_name=length,load_best_model_at_end=False,local_rank=0,log_level=passive,log_level_replica=warning,log_on_each_node=True,logging_dir=output/adgen-chatglm2-6b-pt-128-2e-2/runs/Sep03_10-30-11_dsw-266423-65d8644bfb-xl2rm,logging_first_step=False,logging_nan_inf_filter=True,logging_steps=10,logging_strategy=steps,lr_scheduler_type=linear,max_grad_norm=1.0,max_steps=3000,metric_for_best_model=None,mp_parameters=,no_cuda=False,num_train_epochs=3.0,optim=adamw_hf,optim_args=None,output_dir=output/adgen-chatglm2-6b-pt-128-2e-2,overwrite_output_dir=True,past_index=-1,per_device_eval_batch_size=1,per_device_train_batch_size=1,predict_with_generate=True,prediction_loss_only=False,push_to_hub=False,push_to_hub_model_id=None,push_to_hub_organization=None,push_to_hub_token=<PUSH_TO_HUB_TOKEN>,ray_scope=last,remove_unused_columns=True,report_to=['tensorboard'],resume_from_checkpoint=None,run_name=output/adgen-chatglm2-6b-pt-128-2e-2,save_on_each_node=False,save_safetensors=False,save_steps=1000,save_strategy=steps,save_total_limit=None,seed=42,sharded_ddp=[],skip_memory_metrics=True,sortish_sampler=False,tf32=None,torch_compile=False,torch_compile_backend=None,torch_compile_mode=None,torchdynamo=None,tpu_metrics_debug=False,tpu_num_cores=None,use_ipex=False,use_legacy_prediction_loop=False,use_mps_device=False,warmup_ratio=0.0,warmup_steps=0,weight_decay=0.0,xpu_backend=None,)/home/pai/lib/python3.9/site-packages/datasets/load.py:2072: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.You can remove this warning by passing 'token=None' instead.  warnings.warn(Downloading data files:   0%|                                                                                                           | 0/2 [00:00<?, ?it/s]Downloading data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 21076.90it/s]Extracting data files:   0%|                                                                                                            | 0/2 [00:00<?, ?it/s]Extracting data files: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 71697.50it/s]Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 0 examples [00:00, ? examples/s]Traceback (most recent call last):  File "/home/pai/lib/python3.9/site-packages/datasets/builder.py", line 1949, in _prepare_split_single    num_examples, num_bytes = writer.finalize()  File "/home/pai/lib/python3.9/site-packages/datasets/arrow_writer.py", line 598, in finalize    raise SchemaInferenceError("Please pass `features` or at least one example when writing data")datasets.arrow_writer.SchemaInferenceError: Please pass `features` or at least one example when writing dataThe above exception was the direct cause of the following exception:Traceback (most recent call last):  File "/mnt/workspace/ChatGLM2-6B/ptuning/main.py", line 411, in <module>    main()  File "/mnt/workspace/ChatGLM2-6B/ptuning/main.py", line 98, in main    raw_datasets = load_dataset(  File "/home/pai/lib/python3.9/site-packages/datasets/load.py", line 2136, in load_dataset    builder_instance.download_and_prepare(  File "/home/pai/lib/python3.9/site-packages/datasets/builder.py", line 954, in download_and_prepare    self._download_and_prepare(  File "/home/pai/lib/python3.9/site-packages/datasets/builder.py", line 1049, in _download_and_prepare    self._prepare_split(split_generator, **prepare_split_kwargs)  File "/home/pai/lib/python3.9/site-packages/datasets/builder.py", line 1813, in _prepare_split    for job_id, done, content in self._prepare_split_single(  File "/home/pai/lib/python3.9/site-packages/datasets/builder.py", line 1958, in _prepare_split_single    raise DatasetGenerationError("An error occurred while generating the dataset") from edatasets.builder.DatasetGenerationError: An error occurred while generating the datasetERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 5731) of binary: /home/pai/bin/pythonTraceback (most recent call last):  File "/home/pai/bin/torchrun", line 8, in <module>    sys.exit(main())  File "/home/pai/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper    return f(*args, **kwargs)  File "/home/pai/lib/python3.9/site-packages/torch/distributed/run.py", line 794, in main    run(args)  File "/home/pai/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run    elastic_launch(  File "/home/pai/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__    return launch_agent(self._config, self._entrypoint, list(args))  File "/home/pai/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent    raise ChildFailedError(torch.distributed.elastic.multiprocessing.errors.ChildFailedError: ============================================================main.py FAILED------------------------------------------------------------Failures:  <NO_OTHER_FAILURES>------------------------------------------------------------Root Cause (first observed failure):[0]:  time      : 2023-09-03_10:30:13  host      : dsw-266423-65d8644bfb-xl2rm  rank      : 0 (local_rank: 0)  exitcode  : 1 (pid: 5731)  error_file: <N/A>  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html============================================================